{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeramirez169/DataScience_SGS_Classification/blob/main/models/02_classic_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVe5ljFZZoTt",
        "outputId": "658e1664-f554-4d8f-cf35-53ab89d14079"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'DataScience_SGS_Classification'...\n",
            "remote: Enumerating objects: 82, done.\u001b[K\n",
            "remote: Counting objects: 100% (82/82), done.\u001b[K\n",
            "remote: Compressing objects: 100% (78/78), done.\u001b[K\n",
            "remote: Total 82 (delta 36), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (82/82), 23.95 MiB | 22.29 MiB/s, done.\n",
            "Resolving deltas: 100% (36/36), done.\n",
            "/content/DataScience_SGS_Classification/DataScience_SGS_Classification/DataScience_SGS_Classification\n",
            "Collecting es-core-news-lg==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_lg-3.8.0/es_core_news_lg-3.8.0-py3-none-any.whl (568.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m568.0/568.0 MB\u001b[0m \u001b[31m837.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "# Clonar tu repositorio desde GitHub\n",
        "!git clone https://github.com/jeramirez169/DataScience_SGS_Classification.git\n",
        "%cd DataScience_SGS_Classification\n",
        "\n",
        "# Instalar dependencias necesarias\n",
        "!pip install -q pandas numpy scikit-learn unidecode spacy imbalanced-learn transformers\n",
        "!python -m spacy download es_core_news_lg\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eo2P_u6jZ9bn",
        "outputId": "81a472a5-d4d9-476f-b26c-7d457bbeaf52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/DataScience_SGS_Classification/DataScience_SGS_Classification/DataScience_SGS_Classification/data\n",
            "Archive:  Dataset_SGS_clean.zip\n",
            "  inflating: ./Dataset_SGS_clean.csv  \n",
            "/content/DataScience_SGS_Classification/DataScience_SGS_Classification/DataScience_SGS_Classification\n"
          ]
        }
      ],
      "source": [
        "# Ir a la carpeta data\n",
        "%cd data\n",
        "\n",
        "# Descomprimir el archivo ZIP\n",
        "!unzip \"Dataset_SGS_clean.zip\" -d .\n",
        "\n",
        "# Regresar al directorio raíz del proyecto\n",
        "%cd ..\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, cross_validate\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from xgboost import XGBClassifier\n",
        "import joblib\n",
        "\n",
        "sns.set(style=\"whitegrid\")"
      ],
      "metadata": {
        "id": "1MiE3fO6D6p5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ruta = \"data/Dataset_SGS_clean.csv\"\n",
        "df = pd.read_csv(ruta, encoding=\"utf-8\")\n",
        "\n",
        "print(\"Columnas del dataset:\", df.columns.tolist())\n",
        "print(\"Tamaño del dataset:\", df.shape)\n",
        "\n",
        "# Variables predictoras y objetivo\n",
        "X = df[\"texto_truncado_lematizado\"]\n",
        "y = df[\"Oficina\"]\n",
        "\n",
        "# División train/test estratificada\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.20,\n",
        "    stratify=y,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"Tamaño entrenamiento:\", len(X_train))\n",
        "print(\"Tamaño prueba:\", len(X_test))\n",
        "\n",
        "# Vectorización TF-IDF compartida\n",
        "tfidf = TfidfVectorizer(\n",
        "    ngram_range=(1, 2),\n",
        "    min_df=3,\n",
        "    max_df=0.95,\n",
        "    sublinear_tf=True\n",
        ")\n",
        "\n",
        "X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf.transform(X_test)\n",
        "\n",
        "print(\"Dimensiones TF-IDF (train):\", X_train_tfidf.shape)\n",
        "\n",
        "labels = sorted(y.unique())\n"
      ],
      "metadata": {
        "id": "sKgTwtOnD7fU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SVM"
      ],
      "metadata": {
        "id": "rqLWOGuTEXiJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "svm_model = LinearSVC(\n",
        "    C=1.0,\n",
        "    loss=\"squared_hinge\",\n",
        "    class_weight=\"balanced\",\n",
        "    max_iter=5000,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "rskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)\n",
        "\n",
        "#Cross validation\n",
        "scores_svm = cross_validate(\n",
        "    svm_model,\n",
        "    X_train_tfidf,\n",
        "    y_train,\n",
        "    cv=rskf,\n",
        "    scoring=[\"accuracy\", \"precision_macro\", \"recall_macro\", \"f1_macro\"],\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "print(\"\\nResultados de Cross-Validation SVM (5x3):\")\n",
        "for metric, values in scores_svm.items():\n",
        "    if \"test\" in metric:\n",
        "        print(f\"{metric}: {values.mean():.4f} ± {values.std():.4f}\")\n",
        "\n",
        "#Entrenamiento\n",
        "svm_model.fit(X_train_tfidf, y_train)\n",
        "y_pred_svm = svm_model.predict(X_test_tfidf)\n",
        "\n",
        "print(\"\\nReporte de clasificación SVM:\")\n",
        "print(classification_report(y_test, y_pred_svm))\n",
        "\n",
        "#Matriz de Confusión\n",
        "cm_svm = confusion_matrix(y_test, y_pred_svm, labels=labels)\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.heatmap(cm_svm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=labels, yticklabels=labels)\n",
        "plt.title(\"Matriz de Confusión – SVM\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mxc6bg07EY3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Encoding\n",
        "le = LabelEncoder()\n",
        "y_train_enc = le.fit_transform(y_train)\n",
        "y_test_enc = le.transform(y_test)\n",
        "\n",
        "print(\"\\nMapeo etiqueta → código:\")\n",
        "for cls, code in zip(le.classes_, le.transform(le.classes_)):\n",
        "    print(f\"{cls:25s} -> {code}\")\n",
        "\n",
        "#Cross-Validation XGBoost\n",
        "xgb_model = XGBClassifier(\n",
        "    objective=\"multi:softprob\",\n",
        "    num_class=len(le.classes_),\n",
        "    max_depth=6,\n",
        "    learning_rate=0.1,\n",
        "    n_estimators=300,\n",
        "    subsample=0.9,\n",
        "    colsample_bytree=0.9,\n",
        "    reg_alpha=1.0,\n",
        "    reg_lambda=1.0,\n",
        "    tree_method=\"hist\",         # CPU\n",
        "    eval_metric=\"mlogloss\",\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "scores_xgb = cross_validate(\n",
        "    xgb_model,\n",
        "    X_train_tfidf,\n",
        "    y_train_enc,\n",
        "    cv=rskf,\n",
        "    scoring=[\"accuracy\", \"precision_macro\", \"recall_macro\", \"f1_macro\"],\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "print(\"\\nResultados de Cross-Validation XGBoost (5x3):\")\n",
        "for metric, values in scores_xgb.items():\n",
        "    if \"test\" in metric:\n",
        "        print(f\"{metric}: {values.mean():.4f} ± {values.std():.4f}\")\n",
        "\n",
        "#Entrenamiento\n",
        "print(\"\\nEntrenando XGBoost...\")\n",
        "xgb_model.fit(X_train_tfidf, y_train_enc)\n",
        "\n",
        "y_pred_xgb_enc = xgb_model.predict(X_test_tfidf)\n",
        "y_pred_xgb = le.inverse_transform(y_pred_xgb_enc)\n",
        "\n",
        "print(\"\\nReporte de clasificación XGBoost:\")\n",
        "print(classification_report(y_test, y_pred_xgb))\n",
        "\n",
        "#Matriz de confusión\n",
        "cm_xgb = confusion_matrix(y_test, y_pred_xgb, labels=labels)\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.heatmap(cm_xgb, annot=True, fmt=\"d\", cmap=\"Greens\",\n",
        "            xticklabels=labels, yticklabels=labels)\n",
        "plt.title(\"Matriz de Confusión – XGBoost\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Normalizada\n",
        "cm_xgb_norm = cm_xgb.astype(float) / cm_xgb.sum(axis=1, keepdims=True)\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.heatmap(cm_xgb_norm, annot=True, fmt=\".2f\", cmap=\"Greens\",\n",
        "            xticklabels=labels, yticklabels=labels)\n",
        "plt.title(\"Matriz de Confusión Normalizada – XGBoost\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t2XRctE-Ek22"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}