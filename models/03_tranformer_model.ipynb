{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeramirez169/DataScience_SGS_Classification/blob/main/models/03_tranformer_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVe5ljFZZoTt",
        "outputId": "8e648dce-7732-4b35-807d-6206cb0e7ca0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'DataScience_SGS_Classification' already exists and is not an empty directory.\n",
            "/content/DataScience_SGS_Classification\n",
            "Collecting es-core-news-lg==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_lg-3.8.0/es_core_news_lg-3.8.0-py3-none-any.whl (568.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m568.0/568.0 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "# Clonar tu repositorio desde GitHub\n",
        "!git clone https://github.com/jeramirez169/DataScience_SGS_Classification.git\n",
        "%cd DataScience_SGS_Classification\n",
        "\n",
        "# Instalar dependencias necesarias\n",
        "!pip install -q pandas numpy scikit-learn unidecode spacy imbalanced-learn transformers\n",
        "!python -m spacy download es_core_news_lg\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eo2P_u6jZ9bn",
        "outputId": "850de0a2-abf3-449c-b017-6db900ee5267"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/DataScience_SGS_Classification/data\n",
            "Archive:  Dataset_SGS_clean.zip\n",
            "replace ./Dataset_SGS_clean.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: /content/DataScience_SGS_Classification\n"
          ]
        }
      ],
      "source": [
        "# Ir a la carpeta data\n",
        "%cd data\n",
        "\n",
        "# Descomprimir el archivo ZIP\n",
        "!unzip \"Dataset_SGS_clean.zip\" -d .\n",
        "\n",
        "# Regresar al directorio raíz del proyecto\n",
        "%cd ..\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "import torch\n",
        "import os"
      ],
      "metadata": {
        "id": "YTXsPvTIouyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTw9DpmmbCo8"
      },
      "outputs": [],
      "source": [
        "ruta = \"data/Dataset_SGS_clean.csv\"\n",
        "df = pd.read_csv(ruta, encoding=\"utf-8\")\n",
        "\n",
        "df = df[['Oficina', 'texto_truncado']].rename(columns={\n",
        "    'Oficina': 'label_text',\n",
        "    'texto_truncado': 'text'\n",
        "})\n",
        "\n",
        "label2id = {label: idx for idx, label in enumerate(sorted(df['label_text'].unique()))}\n",
        "id2label = {v: k for k, v in label2id.items()}\n",
        "df['label'] = df['label_text'].map(label2id)\n",
        "\n",
        "print(\"Etiquetas y códigos:\", label2id)\n",
        "print(\"Total de muestras:\", len(df))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train, df_test = train_test_split(\n",
        "    df,\n",
        "    test_size=0.2,\n",
        "    stratify=df['label'],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Tamaño entrenamiento: {len(df_train)}\")\n",
        "print(f\"Tamaño prueba: {len(df_test)}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "5-ToBtS-obah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenización\n",
        "MODEL_NAME = \"dccuchile/bert-base-spanish-wwm-uncased\"\n",
        "MAX_LEN = 256\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "train_ds = Dataset.from_pandas(df_train)\n",
        "test_ds = Dataset.from_pandas(df_test)\n",
        "\n",
        "def tokenize(batch):\n",
        "    return tokenizer(\n",
        "        batch[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=MAX_LEN\n",
        "    )\n",
        "\n",
        "train_ds = train_ds.map(tokenize, batched=True)\n",
        "test_ds = test_ds.map(tokenize, batched=True)\n",
        "\n",
        "cols = [\"input_ids\", \"attention_mask\", \"label\"]\n",
        "train_ds.set_format(\"torch\", columns=cols)\n",
        "test_ds.set_format(\"torch\", columns=cols)\n"
      ],
      "metadata": {
        "id": "YIJLZaGAoi-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cargar modelo BETO\n",
        "NUM_LABELS = len(label2id)\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=NUM_LABELS,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n"
      ],
      "metadata": {
        "id": "Fklhf-nKpIpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calcular pesos de clase (para desbalance)\n",
        "counts = df_train['label'].value_counts().sort_index().values\n",
        "class_weights = torch.tensor(\n",
        "    (counts.sum() / (NUM_LABELS * counts)),\n",
        "    dtype=torch.float\n",
        ")\n",
        "print(\"Pesos de clase:\", class_weights)\n"
      ],
      "metadata": {
        "id": "UhWEEYUUpKRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Función de perdida\n",
        "def custom_loss(model, inputs, return_outputs=False):\n",
        "    labels = inputs.pop(\"labels\")\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights.to(logits.device))\n",
        "    loss = loss_fn(logits, labels)\n",
        "    return (loss, outputs) if return_outputs else loss\n"
      ],
      "metadata": {
        "id": "stYEqpQ8pSeF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Métricas\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = logits.argmax(axis=-1)\n",
        "    macro_f1 = f1_score(labels, preds, average=\"macro\")\n",
        "    weighted_f1 = f1_score(labels, preds, average=\"weighted\")\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        \"accuracy\": acc,\n",
        "        \"macro_f1\": macro_f1,\n",
        "        \"weighted_f1\": weighted_f1\n",
        "    }"
      ],
      "metadata": {
        "id": "_OOr0ZHBpV0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = TrainingArguments(\n",
        "    output_dir=\"./beto_sgs_80_20\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=5,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"macro_f1\",\n",
        "    logging_steps=100,\n",
        "    warmup_ratio=0.1,\n",
        "    weight_decay=0.01,\n",
        "    seed=42\n",
        ")\n"
      ],
      "metadata": {
        "id": "zP1fo2UGpX4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Entrenamiento\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=test_ds,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.compute_loss = custom_loss\n",
        "trainer.train()\n",
        "\n"
      ],
      "metadata": {
        "id": "5xMVcN3-pZhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluación\n",
        "metrics_test = trainer.evaluate(test_ds)\n",
        "print(\"\\nResultados en conjunto de prueba (20%):\")\n",
        "for k, v in metrics_test.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "V8KJI7Eopc_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Matriz de confusión\n",
        "preds = trainer.predict(test_ds)\n",
        "y_true = preds.label_ids\n",
        "y_pred = preds.predictions.argmax(-1)\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred, normalize='true')\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, cmap=\"Blues\", fmt=\".2f\",\n",
        "            xticklabels=id2label.values(),\n",
        "            yticklabels=id2label.values())\n",
        "plt.xlabel(\"Predicción\")\n",
        "plt.ylabel(\"Etiqueta real\")\n",
        "plt.title(\"Matriz de Confusión - BETO SGS (80/20)\")\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "xqbAr8idpeVr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}